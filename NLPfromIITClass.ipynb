{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYUg6UPmaKEIPuCqUkBUHg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3bbinesh/TimeSeries/blob/main/NLPfromIITClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FNrYwg8GpEfh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPMhV78CrEKN",
        "outputId": "c418764d-40fe-463d-b120-b0937c0b90ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\""
      ],
      "metadata": {
        "id": "qAO17HT0sJLS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpjJG0YRsLhz",
        "outputId": "60754c5f-9519-46a2-b020-b77efbffed29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.sub(r'[^\\w\\s]',\"\",text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqxXnPOZu-jb",
        "outputId": "0085b8e5-a743-4571-f30d-b2ce9a296597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example sentence to demonstrate text preprocessing well clean it and tokenize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "englishstop_words=set(stopwords.words('english'))\n",
        "print(englishstop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOMpNvGWydNd",
        "outputId": "09629df2-d17d-4c07-eadd-d780a035fa8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'above', 'hadn', \"shan't\", 'yourselves', 'there', 'has', 'we', 'haven', 'into', \"you've\", 'herself', 'while', 'in', 'mustn', 'between', 'more', 'nor', \"don't\", 'when', \"isn't\", 'until', 'who', 'me', 'be', 'can', 'are', \"hasn't\", 'this', 'him', 'with', 'won', 'am', 'these', \"needn't\", 's', 'no', 'myself', 'out', 'how', 'further', 'just', 'doing', 'or', \"haven't\", \"it's\", 'other', 'being', 've', 'have', 'such', \"couldn't\", 'a', 'her', 'that', 'does', 'do', 'if', 'll', \"hadn't\", \"mustn't\", 'did', 'your', 'all', 'had', 'both', 'only', 'own', \"doesn't\", 'should', 'below', \"you're\", 'been', 'up', 'too', 'yourself', \"wouldn't\", 'which', 'is', 'over', 'before', 'themselves', \"aren't\", 'at', 'then', 'now', 'ma', 'here', 'it', 'hers', 'off', 'because', 'about', 'some', 'don', 'their', 'and', 'ourselves', 'he', 'himself', 'an', 'm', 'were', 't', 'down', 'through', 'yours', 'very', \"you'd\", 'why', 'whom', 'wouldn', 'after', 'our', 'mightn', 'any', 'those', 'where', 'theirs', 'once', 'i', \"she's\", 'under', 'having', 'on', 'most', 'didn', 'my', 'd', 'what', 'each', 'isn', 'during', 'y', 'itself', 'will', 'they', 'the', 'needn', 'not', \"didn't\", 'them', 'weren', 'from', 'so', \"that'll\", \"shouldn't\", \"won't\", 'shouldn', 'for', 'his', 'but', 'wasn', 'again', 'was', 'ours', 'ain', 'same', 'shan', \"should've\", 'by', \"wasn't\", 'its', 'couldn', 'doesn', 'you', 'to', 'hasn', 'than', \"you'll\", 'as', 'o', 'few', \"mightn't\", 'she', 'aren', 're', 'of', \"weren't\", 'against'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text=text.lower()\n",
        "    text=re.sub(r'[^\\w\\s]',\"\",text)\n",
        "    tokens = word_tokenize(text)\n",
        "    englishstop_words=set(stopwords.words('english'))\n",
        "    tokens=[word for word in tokens if word not in englishstop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "C9Cc99h7vWpH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tokens = preprocess(text)\n",
        "print(processed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq41kwI80Zyc",
        "outputId": "0b33c365-9123-4828-fd37-52b0fd7b69b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrate', 'text', 'preprocessing', 'well', 'clean', 'tokenize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "mGA6Y3WIVMJm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\"\n",
        "doc=nlp(text)\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTUuKuZ5VVEx",
        "outputId": "d6f53fb5-91a6-4c6a-ac74-b45ebba50a18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Part of Speec Tagging\")\n",
        "for token in doc:\n",
        "    print(token.text,\":\", token.pos_,\":\",token.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1nuDacOVifV",
        "outputId": "2bf5ad65-5ffc-42e1-d9aa-201a0379008d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speec Tagging\n",
            "Apple : PROPN : NNP\n",
            "is : AUX : VBZ\n",
            "looking : VERB : VBG\n",
            "at : ADP : IN\n",
            "buying : VERB : VBG\n",
            "U.K. : PROPN : NNP\n",
            "startup : NOUN : NN\n",
            "for : ADP : IN\n",
            "$ : SYM : $\n",
            "1 : NUM : CD\n",
            "billion : NUM : CD\n",
            ". : PUNCT : .\n",
            "Steve : PROPN : NNP\n",
            "Jobs : PROPN : NNP\n",
            "founded : VERB : VBD\n",
            "Apple : PROPN : NNP\n",
            "in : ADP : IN\n",
            "1976 : NUM : CD\n",
            ". : PUNCT : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Named Entitiess\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,\":\",ent.label_,\":\",spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgYB6wcdW74G",
        "outputId": "d8405081-003c-45dd-bcef-aee4350eb9e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entitiess\n",
            "Apple : ORG : Companies, agencies, institutions, etc.\n",
            "U.K. : GPE : Countries, cities, states\n",
            "$1 billion : MONEY : Monetary values, including unit\n",
            "Steve Jobs : PERSON : People, including fictional\n",
            "Apple : ORG : Companies, agencies, institutions, etc.\n",
            "1976 : DATE : Absolute or relative dates or periods\n"
          ]
        }
      ]
    }
  ]
}